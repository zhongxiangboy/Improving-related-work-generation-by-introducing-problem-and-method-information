transliteration methods typically fall into two categories :(li et al., 2004);(jung et al., 2000);(knight and graehl, 1998) generative approaches  that try to produce the target transliteration given a source language ne , generative methods encounter the out-of-vocabulary ( oov ) problem and require substantial amounts of training data and knowledge of the source and target languages. (goldwasser and roth, 2008b);(bergsma and kondrak,2007);(sproat et al., 2006);(klementiev and roth, 2006a) and discriminative approaches   , that try to identify the correct transliteration for a word in the source language given several candidates in the target language. discriminative approaches , when used to for discovering ne in a bilingual corpora avoid the oov problem by choosing the transliteration candidates from the corpora. these methods typically make very little assumptions about the source and target languages and require considerably less data to converge. (bergsma and kondrak,2007);(goldwasser and roth, 2008b);(sproat et al., 2006);(klementiev and roth, 2006a) training the transliteration model is typically done under supervised settings  , or weakly supervised settings with additional temporal information . our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the nlp community recently. (roth and yih, 2004);(riedel and clarke, 2006);(haghighi and klein, 2006);(chang et al., 2007) this has been shown both in supervised settings  and unsupervised settings  in which constraints are used to bootstrap the model. (chang et al., 2007) describes an unsupervised training of a constrained conditional model ( ccm ) , a general framework for combining statistical models with declarative constraints. we extend this work to include constraints over possible assignments to latent variables which , in turn , define the underlying representation for the learning problem. (ristad and yianilos, 1998);(bergsma and kondrak,2007);(goldwasser and roth, 2008b) in the transliteration community there are several works  that show how the feature representation of a word pair can be restricted to facilitate learning a string similarity model. (goldwasser and roth, 2008b) we follow the approach discussed in , which considers the feature representation as a structured prediction problem and finds the set of optimal assignments ( or feature activations ) , under a set of legitimacy constraints. this approach stresses the importance of interaction between learning and inference , as the model iteratively uses inference to improve the sample representation for the learning problem and uses the learned model to improve the accuracy of the inference process. we adapt this approach to unsupervised settings , where iterating over the data improves the model in both of these dimensions. 