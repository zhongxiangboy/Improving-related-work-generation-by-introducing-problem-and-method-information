no consensus has been reached with respect to the proper methodology to use when evaluating paraphrase quality. this section reviews past methods for paraphrase evaluation. researchers usually present the quality of their automatic paraphrasing technique in terms of a subjective manual evaluation. these have used a variety of criteria. (barzilay and mckeown 2001) for example , evaluated their paraphrases by asking judges whether paraphrases were " approximately conceptually equivalent. "(ibrahim et al. 2003) asked judges whether their paraphrases were " roughly interchangeable given the genre. "(bannard and callison burch 2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions " preserved meaning and remained grammatical. "these subjective evaluations are rather vaguely defined and not easy to reproduce. others evaluate paraphrases in terms of whether they improve performance on particular tasks. (callison burch et al. 2006b);(papineni et al., 2002) measure improvements in translation quality in terms of bleu score and in terms of subjective human evaluation when paraphrases are integrated into a statistical machine translation system. (lin and pantel 2001) manually judge whether a paraphrase might be used to answer questions from the trec question-answering track. to date , no one has used task-based evaluation to compare different paraphrasing methods. even if such an evaluation were performed , it is unclear whether the results would hold for a different task. because of this , we strive for a general evaluation rather than a task-specific one. (dolan et al. 2004) create a set of manual word alignments between pairs of english sentences. we create a similar type of data , as described in section 4. (dolan et al. 2004) use heuristics to draw pairs of english sentences from a comparable corpus of newswire articles , and treat these as potential paraphrases. in some cases these sentence pairs are good examples of paraphrases , and in some cases they are not. our data differs because it is drawn from multiple translations of the same foreign sentences. (barzilay 2003) suggested that multiple translations of the same foreign source text were a perfect source for " naturally occurring paraphrases " because they are samples of text which convey the same meaning but are produced by different writers. (dolan et al. 2004) that being said , it may be possible to use ' data toward a similar end. (cohn et al. 2008) compares the use of the multiple translation corpus with the msr corpus for this task. the work described here is similar to work in summarization evaluation. (nenkova et al., 2007) for example , in the pyramid method content units that are similar across human-generated summaries are hand-aligned. these can have alternative wordings , and are manually grouped. the idea of capturing these and building a resource for evaluating summaries is in the same spirit as our methodology. 