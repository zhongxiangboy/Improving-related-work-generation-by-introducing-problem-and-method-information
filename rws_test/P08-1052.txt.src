0	training and scaling preference functions for disambiguation . we present an automatic method for weighting the contributions of preference functions used in disambiguation .initial scaling factors are derived as the solution to a least-squares minimization problem , and improvements are then made by hill-climbing .the method is applied to disambiguating sentences in the atis ( air travel information system corpus , and the performance of the resulting scaling factors is compared with hand-tuned factors .we then focus on one class of preference function , those based on semantic lexical collocations .experimental results are presented showing that such functions vary considerably in selecting correct analyses .in particular we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations . least - squares minimization problem based on automatic method
1	semi-automatic recognition of noun modifier relationships . semantic relationships among words and phrases are often marked by explicit syntactic or lexical clues that help recognize such relationships in texts .within complex nominals , however , few overt clues are available .systems that analyze such nominals must compensate for the lack of surface clues with other information .one way is to load the system with lexical semantics for nouns or adjectives .this merely shifts the problem elsewhere : how do we define the lexical semantics and build large semantic lexicons ?another way is to find constructions similar to a given complex nominal , for which the relationships are already known .this is the way we chose , but it too has drawbacks .similarity is not easily assessed , similar analyzed constructions may not exist , and if they do exist , their analysis may not be appropriate for the current nominal .we present a semi-automatic system that identifies semantic relationships in noun phrases without using precoded noun or adjective semantics .instead , partial matching on previously analyzed noun phrases leads to a tentative interpretation of a new input .processing can start without prior analyses , but the early stage requires user interaction .as more noun phrases are analyzed , the system learns to find better interpretations and reduces its reliance on the user .in experiments on english technical texts the system correctly identified 60-70 % of relationships automatically . semi - automatic recognition of noun modifier relationships based on semi - automatic system
2	on the semantics of noun compounds . this paper provides new insights on the semantic characteristics of two and three noun compounds .an analysis is performed using two sets of semantic classification categories : a list of 8 prepositional paraphrases previously proposed by lauer [ designing statistical language learners : experiments on noun compounds , ph.d.thesis , macquarie university , australia ] and a new set of 35 semantic relations introduced by us .we show the distribution of these semantic categories on a corpus of noun compounds and present several models for the bracketing and the semantic classification of noun compounds .the results are compared against state-of-the-art models reported in the literature . semantic classification of noun compounds based on semantic classification categories
3	generalizing automatically generated selectional patterns . frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus ; tins information can then serve as the basis for selectional constraints when analyzing new text from the same domain .this information , however , is necessarily incomplete .we report on measurements of the degree of selectional coverage obtained with different sizes of corpora .we then describe a technique for using the corpus to identify selectionally similar terms , and for using this similarity to broaden the selectional coverage for a fixed corpus size .  based on 
4	interpreting semantic relations in noun compounds via verb semantics . we propose a novel method for automatically interpreting compound nouns based on a predefined set of semantic relations .first we map verb tokens in sentential contexts to a fixed set of seed verbs using wordnet : : similarity and moby ï¿½ s thesaurus .we then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier .based on the semantics of the matched sentences , we then build a classifier using timbl .the performance of our final system at interpreting ncs is 52.6 % . automatically interpreting compound nouns based on classifier
5	web-based models for natural language processing . previous work demonstrated that web counts can be used to approximate bigram counts , thus suggesting that web-based frequencies should be useful for a wide variety of nlp tasks .however , only a limited number of tasks have so far been tested using web-scale data sets .the present paper overcomes this limitation by systematically investigating the performance of web-based models for several nlp tasks , covering both syntax and semantics , both generation and analysis , and a wider range of n-grams and parts of speech than have been previously explored .for the majority of our tasks , we find that simple , unsupervised models perform better when n-gram counts are obtained from the web rather than from a large corpus .in some cases , performance can be improved further by using backoff or interpolation techniques that combine web counts and corpus counts .however , unsupervised web-based models generally fail to outperform supervised state-of-the-art models trained on smaller corpora .we argue that web-based models should therefore be used as a baseline for , rather than an alternative to , standard supervised models . natural language processing based on unsupervised web - based models
6	an information-theoretic definition of similarity . similarity is an important and widely used concept .previous definitions of similarity are tied to a particular application or a form of knowledge representation .we present an information- theoretic definition of similarity that is applicable as long as there is a probabilistic model .we demonstrate how our definition can be used to measure the similarity in a number of different domains .  based on knowledge representation
7	discovery of inference rules for question answering . one of the main challenges in question-answering is the potential mismatch between the expressions in questions and the expressions in texts .while humans appear to use inference rules such as " x writes y " implies " x is the author of y " in answering questions , such rules are generally unavailable to question-answering systems due to the inherent difficulty in constructing them .in this paper , we present an unsupervised algorithm for discovering inference rules from text .our algorithm is based on an extended version of harris ' distributional hypothesis , which states that words that occurred in the same contexts tend to be similar .instead of using this hypothesis on words , we apply it to paths in the dependency trees of a parsed corpus .essentially , if two paths tend to link the same set of words , we hypothesize that their meanings are similar .we use examples to show that our system discovers many inference rules easily missed by humans . question - answering based on question - answering systems
8	using verbs to characterize noun-noun relations . we present a novel , simple , unsupervised method for characterizing the semantic relations that hold between nouns in noun-noun compounds .the main idea is to discover predicates that make explicit the hidden relations between the nouns .this is accomplished by writing web search engine queries that restate the noun compound as a relative clause containing a wildcard character to be filled in with a verb .a comparison to results from the literature suggest this is a promising approach .  based on web search engine queries
9	citances : citation sentences for semantic analysis of bioscience text . we propose the use of the text of the sentences surrounding citations as an important tool for semantic interpretation of bioscience text .we hypothesize several different uses of citation sentences ( which we call citances ) , including the creation of training and testing data for semantic analysis ( especially for entity and relation recognition ) , synonym set creation , database curation , document summarization , and information retrieval generally .we illustrate some of these ideas , showing that citations to one document in particular align well with what a hand-built curator extracted .we also show preliminary results on the problem of normalizing the different ways that the same concepts are expressed within a set of citances , using and improving on existing techniques in automatic paraphrase generation . semantic interpretation of bioscience text based on hand - built curator
10	exploring noun-modifier semantic relations . we explore the semantic similarity between base noun phrases in clusters determined by a comprehensive set of semantic relations .the attributes that characterize modifiers and nouns are extracted from wordnet and from roget 's thesaurus .we use various machine learning tools to find combinations of attributes that explain the similarities in each category .the experiments gave promising results , with a good level of generalization and interesting sets of rules .  based on machine learning tools
11	classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy . we are developing corpus-based techniques for identifying semantic relations at an intermediate level of description ( more specific than those used in case frames , but more general than those used in traditional knowledge representation systems ) .in this paper we describe a classification algorithm for identifying relationships between two-word noun compounds .we find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves .  based on knowledge representation systems
12	the descent of hierarchy , and selection in relational semantics . in many types of technical texts , meaning is embedded in noun compounds .a language understanding program needs to be able to interpret these in order to ascertain sentence meaning .we explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories , and then using this category membership to determine the relation that holds between the nouns .in this paper we present the results of an analysis of this method on two- word noun compounds from the biomedical domain , obtaining classification accuracy of approximately 90 % .since lexical hierarchies are not necessarily ideally suited for this task , we also pose the question : how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question ?we find that the topmost levels of the hierarchy yield an accurate classification , thus providing an economic way of assigning relations to noun compounds . descent of hierarchy based on language understanding program
13	automatic paraphrase acquisition from news articles . paraphrases play an important role in the variety and complexity of natural language documents .however they adds to the difficulty of natural language processing .here we describe a procedure for obtaining paraphrases from news article .a set of paraphrases can be useful for various kinds of applications .articles derived from different newspapers can contain paraphrases if they report the same event of the same day .we exploit this feature by using named entity recognition .our basic approach is based on the assumption that named entities are preserved across paraphrases .we applied our method to articles of two domains and obtained notable examples .although this is our initial attempt to automatically extracting paraphrases from a corpus , the results are promising . automatically extracting paraphrases based on 
14	similarity of semantic relations . there are at least two kinds of similarity .relational similarity is correspondence between relations , in contrast with attributional similarity , which is correspondence between attributes .when two words have a high degree of attributional similarity , we call them synonyms .when two pairs of words have a high degree of relational similarity , we say that their relations are analogous .for example , the word pair mason : stone is analogous to the pair carpenter : wood .this paper introduces latent relational analysis ( lra ) , a method for measuring relational similarity .lra has potential applications in many areas , including information extraction , word sense disambiguation , and information retrieval .recently the vector space model ( vsm ) of information retrieval has been adapted to measuring relational similarity , achieving a score of 47 % on a collection of 374 college-level multiple-choice word analogy questions .in the vsm approach , the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus .lra extends the vsm approach in three ways : ( 1 ) the patterns are derived automatically from the corpus , ( 2 ) the singular value decomposition ( s vd ) is used to smooth the frequency data , and ( 3 ) automatically generated synonyms are used to explore variations of the word pairs .lra achieves 56 % on the 374 analogy questions , statistically equivalent to the average human score of 57 % .on the related problem of classifying semantic relations , lra achieves similar gains over the vsm . measuring relational similarity based on singular value decomposition ( s vd
15	measuring semantic similarity by latent relational analysis . this paper introduces latent relational analysis ( lra ) , a method for measuring semantic similarity .lra measures similarity in the semantic relations between two pairs of words .when two pairs have a high degree of relational similarity , they are analogous .for example , the pair cat : meow is analogous to the pair dog : bark .there is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks ( e.g. , analogical reasoning ) .in the vector space model ( vsm ) approach to measuring relational similarity , the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs .the elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus .lra extends the vsm approach in three ways : ( 1 ) patterns are derived automatically from the corpus , ( 2 ) singular value decomposition is used to smooth the frequency data , and ( 3 ) synonyms are used to reformulate word pairs .this paper describes the lra algorithm and experimentally compares lra to vsm on two tasks , answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions .lra achieves state-of-the-art results , reaching human-level performance on the analogy questions and significantly exceeding vsm performance on both tasks . measuring relational similarity based on vector space model ( vsm ) approach
16	expressing implicit semantic relations without supervision . we present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .for a given input word pair x : y with some unspecified semantic relations , the corresponding output list of patterns p1 , ... , pm is ranked according to how well each pattern pi expresses the relations between x and y. for example , given x = ostrich and y = bird , the two highest ranking output patterns are x is the largest y and y such as the x. the output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .the patterns are sorted by pertinence , where the pertinence of a pattern pi for a word pair x : y is the expected relational similarity between the given pair and typical pairs for pi .the algorithm is empirically evaluated on two tasks , solving multiple-choice sat word analogy questions and classifying semantic relations in noun-modifier pairs .on both tasks , the algorithm achieves state- of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf . multiple - choice sat word analogy questions based on unsupervised learning algorithm
17	corpus-based learning of analogies and semantic relations . we present an algorithm for learning from unlabeled text , based on the vector space model ( vsm ) of information retrieval , that can solve verbal analogy questions of the kind found in the sat college entrance exam .a verbal analogy has the form a : b : : c : d , meaning a is to b as c is to d ; for example , mason : stone : : carpenter : wood .sat analogy questions provide a word pair , a : b , and the problem is to select the most analogous word pair , c : d , from a set of five choices .the vsm algorithm correctly answers 47 % of a collection of 374 college- level analogy questions ( random guessing would yield 20 % correct ; the average college-bound senior high school student answers about 57 % correctly ) .we motivate this research by applying it to a difficult problem in natural language processing , determining semantic relations in noun-modifier pairs .the problem is to classify a noun-modifier pair , such as laser printer , according to the semantic relation between the noun ( printer ) and the modifier ( laser ) .we use a supervised nearest- neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data .with 30 classes of semantic relations , on a collection of 600 labeled noun-modifier pairs , the learning algorithm attains an f value of 26.5 % ( random guessing : 3.3 % ) .with 5 classes of semantic relations , the f value is 43.2 % ( random : 20 % ) .the performance is state-of-the-art for both verbal analogies and noun-modifier relations . - based learning of analogies based on supervised nearest - neighbour algorithm
