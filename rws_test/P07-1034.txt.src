0	domain adaptation with structural correspondence learning . discriminative learning methods are widely used in natural language processing .these methods work best when their training and test data are drawn from the same distribution .for many nlp tasks , however , we are confronted with new domains in which labeled data is scarce or non-existent .in such cases , we seek to adapt existing models from a resource- rich source domain to a resource-poor target domain .we introduce structural correspondence learning to automatically induce correspondences among features from different domains .we test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data , as well as improvements in target domain parsing accuracy using our improved tagger . natural language processing based on structural correspondence learning
1	adaptation of maximum entropy capitalizer : little data can help a lot ciprian chelba and alex acero microsoft research one microsoft way redmond , wa 98052 { chelba , alexac } @ microsoft.com abstract a novel technique for maximum � a posteriori � ( map ) adaptation of maximum entropy ( maxent ) and maximum entropy markov models ( memm ) is presented . adaptation of maximum entropy capitalizer based on maximum entropy markov models
2	domain adaptation for statistical classifiers . the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution .unfortunately , in many applications , the in-domain test data is drawn from a distribution that is related , but not identical , to the out-of-domain distribution of the training data .we consider the common case in which labeled out-of-domain data is plentiful , but labeled in-domain data is scarce .we introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts .we present efficient inference algorithms for this special case based on the technique of conditional expectation maximization .our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain . conditional expectation maximization based on statistical learning theory
3	a statistical model for multilingual entity detection and tracking . entity detection and tracking is a relatively new addition to the repertoire of natural language tasks .in this paper , we present a statistical language-independent framework for identifying and tracking named , nominal and pronominal references to entities within unrestricted text documents , and chaining them into clusters corresponding to each logical entity present in the text .both the mention detection model and the novel entity tracking model can use arbitrary feature types , being able to integrate a wide array of lexical , syntactic and semantic features .in addition , the mention detection model crucially uses feature streams derived from different named entity classifiers .the proposed framework is evaluated with several experiments run in arabic , chinese and english texts ; a system based on the approach described here and submitted to the latest automatic content extraction ( ace ) evaluation achieved top-tier results in all three evaluation languages . automatic content extraction ( ace ) evaluation based on statistical language - independent framework
4	supervised and unsupervised pcfg adaptation to novel domains . this paper investigates adapting a lexicalized probabilistic context-free grammar ( pcfg ) to a novel domain , using maximum a posteriori ( map ) estimation .the map framework is general enough to include some previous model adaptation approaches , such as corpus mixing in gildea ( 2001 ) , for example .other approaches falling within this framework are more effective .in contrast to the results in gildea ( 2001 ) , we show f-measure parsing accuracy gains of as much as 2.5 % for high accuracy lexicalized parsing through the use of out-of-domain treebanks , with the largest gains when the amount of in- domain data is small .map adaptation can also be based on either supervised or unsupervised adaptation data .even when no in-domain treebank is available , unsupervised techniques provide a substantial accuracy gain over unadapted grammars , as much as nearly 5 % f-measure improvement . accuracy lexicalized parsing based on supervised and unsupervised pcfg adaptation
