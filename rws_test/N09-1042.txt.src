0	catching the drift : probabilistic content models , with applications to generation and summarization . we consider the problem of modeling the content structure of texts within a specific domain , in terms of the topics the texts address and the order in which these topics appear .we first present an effective knowledge-lean method for learning content models from unannotated documents , utilizing a novel adaptation of algorithms for hidden markov models .we then apply our method to two complementary tasks : information ordering and extractive summarization .our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods . generation and summarization based on probabilistic content models
1	inferring strategies for sentence ordering in multidocument news summarization abstract . while sentence ordering for single document summarization can be determined from the ordering of sentences in the input article , this is not the case for multidocument summarization where summary sentences may be drawn from different input articles .in this paper , we propose a methodology for studying the properties of ordering information in the news genre and describe experiments done on a corpus of multiple acceptable orderings we developed for the task .based on these experiments , we implemented a strategy for ordering information that combines constraints from chronological order of events and topical relatedness .evaluation of our augmented algorithm shows a significant improvement of the ordering over two baseline strategies . single document summarization based on inferring strategies
2	latent dirichlet allocation . we describe latent dirichlet allocation ( lda ) , a generative probabilistic model for collections of discrete data such as text corpora .lda is a three-level hierarchical bayesian model , in which each item of a collection is modeled as a finite mixture over an underlying set of topics .each topic is , in turn , modeled as an infinite mixture over an underlying set of topic probabilities .in the context of text modeling , the topic probabilities provide an explicit representation of a document .we present efficient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation .we report results in document modeling , text classification , and collaborative filtering , comparing to a mixture of unigrams model and the probabilistic lsi model . collaborative filtering based on three - level hierarchical bayesian model
3	a unified local and global model for discourse coherence . note to readers : we have recently detected a software bug which affects the results of our standalone entity grid experiments . ( the bug was in our syntactic analysis code , which incorrectly failed to label the second object of a conjoint vp ; in the phrase wash the dishes and clean the sink , dishes would be correctly labeled as o but sink mislabeled as x. )this bug happened to have an unfortunate interaction with the this is preliminary information preamble mentioned in section 5 .the results in table 2 above the line are incorrect ; our relaxed entity grid does not outperform the naive grid on the discriminative test .this implies that our argument motivating the relaxed model at the end of section 2 is misguided .the design and performance of the joint model is unaffected .we present a model for discourse coherence which combines the local entity- based approach of ( barzilay and lapata , 2005 ) and the hmm-based content model of ( barzilay and lee , 2004 ) .unlike the mixture model of ( soricut and marcu , 2006 ) , we learn local and global features jointly , providing a better theoretical explanation of how they are useful .as the local component of our model we adapt ( barzilay and lapata , 2005 ) by relaxing independence assumptions so that it is effective when estimated generatively .our model performs the ordering task competitively with ( soricut and marcu , 2006 ) , and significantly better than either of the models it is based on . discourse coherence based on unified local and global model
4	finding scientific topics . we describe a generative model for documents , introduced by blei , ng , and jordan [ blei , d. m. , ng , a. y. & jordan , m. i. ( 2003 ) j. machine learn . res. 3 , 993-1022 ] , in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution .we then present a markov chain monte carlo algorithm for inference in this model .we use this algorithm to analyze abstracts from pnas by using bayesian model selection to establish the number of topics .we show that the extracted topics capture meaningful structure in the data , consistent with the class designations provided by the authors of the articles , and outline further applications of this analysis , including identifying hot topics by examining temporal dynamics and tagging abstracts to illustrate semantic content . . machine learn based on markov chain monte carlo algorithm
5	integrating topics and syntax . statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words .we present a generative model that uses both kinds of dependencies , and is capable of simultaneously finding syntactic classes and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency .this model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long- range dependencies respectively . document classification based on statistical approaches
6	hidden topic markov models . algorithms such as latent dirichlet allocation ( lda ) have achieved significant progress in modeling word document relationships .these algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document .given these parameters , the topics of all words in the same document are assumed to be independent .in this paper , we propose modeling the topics of words in the document as a markov chain .specifically , we assume that all words in the same sentence have the same topic , and successive sentences are more likely to have the same topics .since the topics are hidden , this leads to using the well-known tools of hidden markov models for learning and inference .we show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics .quantitatively , we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity . inference based on latent dirichlet allocation
7	evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus . we use a reliably annotated corpus to compare metrics of coherence based on centering theory with respect to their potential usefulness for text structuring in natural language generation .previous corpus-based evaluations of the coherence of text according to centering did not compare the coherence of the chosen text structure with that of the possible alternatives .a corpus- based methodology is presented which distinguishes between centering-based metrics taking these alternatives into account , and represents therefore a more appropriate way to evaluate centering from a text structuring perspective . natural language generation based on corpus - based methodology
8	probabilistic text structuring : experiments with sentence ordering . ordering information is a critical task for natural language generation applications .in this paper we propose an approach to information ordering that is particularly suited for text-to-text generation .we describe a model that learns constraints on sentence order from a corpus of domain- specific texts and an algorithm that yields the most likely order among several alternatives .we evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model ï¿½ s task .we also assess the appropriateness of such a model for multidocument summarization . natural language generation applications based on probabilistic text structuring
9	unsupervised topic modelling for multi-party spoken discourse . we present a method for unsupervised topic modelling which adapts methods used in document classification ( blei et al. , 2003 ; griffiths and steyvers , 2004 ) to unsegmented multi-party discourse transcripts .we show how bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .we also show that this method appears robust in the face of off-topic dialogue and speech recognition errors . document classification based on unsupervised segmentation - only methods
10	modeling online reviews with multi-grain topic models . in this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews .extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews [ 18 , 19 , 7 , 12 , 27 , 36 , 21 ] .our models are based on extensions to standard topic modeling methods such as lda and plsa to induce multi-grain topics .we argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects ( e.g. , the brand of a product type ) rather than the aspects of an object that tend to be rated by a user .the models we present not only extract ratable aspects , but also cluster them into coherent topics , e.g. , waitress and bartender are part of the same topic staff for restaurants .this differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering .we evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models . generating opinion - based summaries of user reviews based on multi - grain topic models
11	topic modeling : beyond bag-of-words . some models of textual corpora employ text generation methods involving n-gram statistics , while others use latent topic variables inferred using the bag-of-words assumption , in which word order is ignored .previously , these methods have not been combined .in this work , i explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical dirichlet bigram language model .the model hyperparameters are inferred using a gibbs em algorithm .on two data sets , each of 150 documents , the new model exhibits better predictive accuracy than either a hierarchical dirichlet bigram language model or a unigram topic model .additionally , the inferred topics are less dominated by function words than are topics discovered using unigram statistics , potentially making them more meaningful .  based on hierarchical dirichlet bigram language model
