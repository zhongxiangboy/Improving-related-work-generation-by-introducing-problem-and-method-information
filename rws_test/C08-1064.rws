there are several other useful approaches to scaling translation models. (zens and ney 2007) remove constraints imposed by the size of main memory by using an external data structure. (johnson et al. 2007) substantially reduce model size with a filtering method. however , neither of these approaches addresses the preprocessing bottleneck. (callison-burch et al. 2005);(zhang and vogel 2005) to our knowledge , the strand of research initiated by and and extended here is the first to do so. (dyer et al. 2008) address this bottleneck with a promising approach based on parallel processing , showing reductions in real time that are linear in the number of cpus. however , they do not reduce the overall cpu time. our techniques also benefit from parallel processing , but they reduce overall cpu time , thus comparing favorably even in this scenario.moreover , our method works even with limited parallel processing. although we saw success with this approach , there are some interesting open problems. as discussed in 4.2 , there are tradeoffs in the form of slower decoding and increased memory usage. (zhang and vogel 2005) decoding speed might be partially addressed using a mixture of online and offline computation as in , but faster algorithms are still needed. memory use is important in nondistributed systems since our data structures will compete with the language model for memory. (navarro and makinen, 2007) it may be possible to address this problem with a novel data structure known as a compressed selfindex , which supports fast pattern matching on a representation that is close in size to the information-theoretic minimum required by the data. our approach is currently limited by the requirement for very fast parameter estimation. as we saw , this appears to prevent us from computing the target-to-source probabilities. it would also appear to limit our ability to use discriminative training methods , since these tend to be much slower than the analytical maximum likelihood estimate. discriminative methods are desirable for feature-rich models that we would like to explore with pattern matching. (chan et al. 2007);(carpuat and wu 2007) for example , and improve translation accuracy using discriminatively trained models with contextual features of source phrases. their features are easy to obtain at runtime using our approach , which finds source phrases in context. however , to make their experiments tractable , they trained their discriminative models offline only for the specific phrases of the test set. combining discriminative learning with our approach is an open problem. 