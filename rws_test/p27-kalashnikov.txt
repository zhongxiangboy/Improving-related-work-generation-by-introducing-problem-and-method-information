grouping search-engine returned citations for person-name queries . we present a technique to group search-engine returned citations for person-name queries , such that the search-engine returned citations in each group belong to the same person .to group the returned citations , we use a multi-faceted approach that considers evidence from three facets : ( 1 ) attributes , ( 2 ) links , and ( 3 ) page similarity .based on the three facets , we construct a relatedness confidence matrix for pairs of citations .we then merge pairs whose matching confidence value is above an empirically determined threshold .experimental results from the implementation of our multi-faceted approach are promising . group search - engine returned citations based on grouping search - engine
web based linkage . when a variety of names are used for the same real-world entity , the problem of detecting all such variants has been known as the ( record ) linkage or entity resolution problem .in this paper , toward this problem , we propose a novel approach that uses the web as the collective knowledge source in addition to contents of entities .our hypothesis is that if an entity e1 is a duplicate of another entity e2 , and if e1 frequently appears together with information i on the web , then e2 may appear frequently with i on the web .by using search engines , we analyze the frequency , urls , or contents of the returned web pages to capture the information i of an entity .extensive experiments verify that our hypothesis holds in many real settings , and the idea of using the web as the additional source for the linkage problem is promising .our proposal shows 51 % ( on average ) and 193 % ( at best ) improvement in precision / recall compared to a baseline approach . entity resolution problem based on search engines
psnus : web people name disambiguation by simple clustering with rich features . we describe about the system description of the psnus team for the semeval-2007 web people search task .the system is based on the clustering of the web pages by using a variety of features extracted and generated from the data provided .this system achieves fa = 0.5 = 0.75 and fa = 0.2 = 0.78 for the final test data set of the task . semeval - 2007 web people search task based on clustering
computing semantic relatedness using wikipedia-based explicit semantic analysis . computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge .we propose explicit semantic analysis ( esa ) , a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from wikipedia .we use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of wikipedia-based concepts .assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics ( e.g. , cosine ) .compared with the previous state of the art , using esa results in substantial improvements in correlation of computed relatedness scores with human judgments : from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts .importantly , due to the use of natural concepts , the esa model is easy to explain to human users . computing semantic relatedness based on machine learning techniques
web people search via connection analysis . nowadays , searches for the web pages of a person with a given name constitute a notable fraction of queries to web search engines .such a query would normally return web pages related to several namesakes , who happened to have the queried name , leaving the burden of disambiguating and collecting pages relevant to a particular person ( from among the namesakes ) on the user .in this paper , we develop a web people search approach that clusters web pages based on their association to different people .our method exploits a variety of semantic information extracted from web pages , such as named entities and hyperlinks , to disambiguate among namesakes referred to on the web pages .we demonstrate the effectiveness of our approach by testing the efficacy of the disambiguation algorithms and its impact on person search . person search based on web people search approach
domain-independent data cleaning via analysis of entity-relationship graph . specifically , we consider a situation where entities in the database are referred to using descriptions ( e.g. , a set of instantiated attributes ) .the objective of reference disambiguation is to identify the unique entity to which each description corresponds .the key difference between the approach we propose ( called reldc ) and the traditional techniques is that reldc analyzes not only object features but also inter-object relationships to improve the disambiguation quality .our extensive experiments over two real data sets and over synthetic datasets show that analysis of relationships significantly improves quality of the result . domain - independent data cleaning based on analysis of entity - relationship graph
exploiting relationships for domain-independent data cleaning * t . in this paper , we address the problem of reference disambiguation .specifically , we consider a situation where entities in the database are referred to using descriptions ( e.g. , a set of instantiated attributes ) .the objective of reference disambiguation is to identify the unique entity to which each description corresponds .the key difference between the approach we propose ( called reldc ) and the traditional techniques is that reldc analyzes not only object features but also inter-object relationships to improve the disambiguation quality .our extensive experiments over two real datasets and over synthetic datasets show that analysis of relationships significantly improves quality of the result . domain - independent data cleaning based on reldc
disambiguation algorithm for people search on the web . in this paper we develop a disambiguation algorithm and then study its impact on people search .the proposed algorithm first uses extraction techniques to automatically extract significant entities such as the names of other persons , organizations , and locations on each webpage .in addition , it extracts and parses html and web related data on each webpage , such as hyperlinks and email addresses .the algorithm then views all this information in a unified way : as an entity-relationship graph where entities ( e.g. , people , organizations , locations , webpages ) are interconnected via relationships ( e.g. , webpage-mentions-person , relationships derived from hyperlinks , etc ) .the algorithm gains its power by being able to analyze several types of information : attributes associated with the entities ( e.g. , tf / idf for webpages ) and , most importantly , direct and indirect interconnections that exist among entities in the er graph . people search based on disambiguation algorithm
improving author coreference by resource-bounded information gathering from the web . accurate entity resolution is sometimes impossible simply due to insufficient information .for example , in research paper author name resolution , even clever use of venue , title and co-authorship relations are often not enough to make a confident coreference decision .this paper presents several methods for increasing accuracy by gathering and integrating additional evidence from the web .we formulate the coreference problem as one of graph partitioning with discriminatively-trained edge weights , and then incorporate web information either as additional features or as additional nodes in the graph .since the web is too large to incorporate all its data , we need an efficient procedure for selecting a subset of web queries and data .we formally describe the problem of resource bounded information gathering in each of these contexts , and show significant accuracy improvement with low cost . resource bounded information gathering based on 
the semeval-2007 weps evaluation : establishing a benchmark for the web people search task . this paper presents the task definition , resources , participation , and comparative results for the web people search task , which was organized as part of the semeval-2007 evaluation exercise .this task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name . semeval - 2007 weps evaluation based on 
semantic integration in text : from ambiguous names to identifiable entities . intelligent access to information requires semantic integration of structured databases with unstructured textual resources .while the semantic integration problem has been widely studied in the database domain on structured data , it has not been fully recognized nor studied on unstructured or semi-structured textual resources .this paper presents a first step towards this goal by studying semantic integration in natural language texts identifying whether different mentions of real world entities , within and across documents , actually represent the same concept .we present a machine learning study of this problem .the first approach is a discriminative approach a pairwise local classifier is trained in a supervised way to determine whether two given mentions represent the same real world entity .this is followed , potentially , by a global clustering algorithm that uses the classifier as its similarity metric .our second approach is a global generative model , at the heart of which is a view on how documents are generated and how names ( of different entity types ) are sprinkled into them .in its most general form , our model assumes : ( 1 ) a joint distribution over entities ( e.g. , a document that mentions president kennedy is more likely to mention oswald or white house than roger clemens ) , ( 2 ) an author model , that assumes that at least one mention of an entity in a document is easily identifiable , and then generates other mentions via ( 3 ) an appearance model , governing how mentions are transformed from the representative mention .we show that both approaches perform very accurately , in the range of 90 % 95 % fl measure for different entity types , much better than previous approaches to ( some aspects of ) this problem . semantic integration problem based on global clustering algorithm
polyphonet : an advanced social network extraction system from the web . social networks play important roles in the semantic web : knowledge management , information retrieval , ubiquitous computing , and so on .we propose a social network extraction system called polyphonet , which employs several advanced techniques to extract relations of persons , detect groups of persons , and obtain keywords for a person .search engines , especially google , are used to measure co-occurrence of information and obtain web documents .several studies have used search engines to extract social networks from the web , but our research advances the following points : first , we reduce the related methods into simple pseudocodes using google so that we can build up integrated systems .second , we develop several new algorithms for social networking mining such as those to classify relations into categories , to make extraction scalable , and to obtain and utilize person-to-word relations .third , every module is implemented in polyphonet , which has been used at four academic conferences , each with more than 500 participants .we overview that system .finally , a novel architecture called super social network mining is proposed ; it utilizes simple modules using google and is characterized by scalability and relate-identify processes : identification of each entity and extraction of relations are repeated to obtain a more precise social network . relate - identify processes based on social network extraction system
self-tuning in graph-based reference disambiguation . nowadays many data mining / analysis applications use the graph analysis techniques for decision making .many of these techniques are based on the importance of relationships among the interacting units .a number of models and measures that analyze the relationship importance ( link structure ) have been proposed ( e.g. , centrality , importance and page rank ) and they are generally based on intuition , where the analyst intuitively decides a reasonable model that fits the underlying data .in this paper , we address the problem of learning such models directly from training data .specifically , we study a way to calibrate a connection strength measure from training data in the context of reference disambiguation problem .experimental evaluation demonstrates that the proposed model surpasses the best model used for reference disambiguation in the past , leading to better quality of reference disambiguation . graph - based reference disambiguation based on graph analysis techniques
person resolution in person search results : webhawk . finding information about people on the web using a search engine is difficult because there is a many-to-many mapping between person names and specific persons ( i.e. referents ) .this paper describes a person resolution system , called webhawk .given a list of pages obtained by submitting a person query to a search engine , webhawk facilitates person search in three steps : first of all , a filter removes those pages that contain no information about any person .secondly , a cluster groups the remaining pages into different clusters , each for one specific person .to make the resulting clusters more meaningful , an extractor is used to induce query-oriented personal information from each page .finally , a namer generates an informative description for each cluster so that users can find any specific person easily .the architecture of webhawk is presented , and the four components are discussed in detail , with a separate evaluation of each component presented where appropriate .a user study shows that webhawk complements most existing search engines and successfully improves users experience of person search on the web . person resolution based on person resolution system
a testbed for people searching strategies in the www . this paper describes the creation of a testbed to evaluate people searching strategies on the world-wide-web .this task involves resolving person names ambiguity and locating relevant information characterising every individual under the same name .motivation . locating relevant information based on people searching strategies
disambiguating web appearances of people in a social network . say you are looking for information about a particular person .a search engine returns many pages for that persons name but which pages are about the person you care about , and which are about other people who happen to have the same name ?furthermore , if we are looking for multiple people who are related in some way , how can we best leverage this social network ?this paper presents two unsupervised frameworks for solving this problem : one based on link structure of the web pages , another using agglomerative / conglomerative double clustering ( a / cdc ) an application of a recently introduced multi-way distributional clustering method .to evaluate our methods , we collected and hand-labeled a dataset of over 1000 web pages retrieved from google queries on 12 personal names appearing together in someones in an email folder .on this dataset our methods outperform traditional agglomerative clustering by more than 20 % , achieving over 80 % f-measure . hand - based on agglomerative / conglomerative double clustering
extracting key phrases to disambiguate personal names on the web . when you search for information regarding a particular person on the web , a search engine returns many pages .some of these pages may be for people with the same name .how can we disambiguate these different people with the same name ?this paper presents an unsupervised algorithm which produces key phrases for the different people with the same name .these key phrases could be used to further narrow down the search , leading to more person specific unambiguous information .the algorithm we propose does not require any biographical or social information regarding the person .although there are some previous work in personal name disambiguation on the web , to our knowledge , this is the first attempt to extract key phrases to disambiguate the different persons with the same name .to evaluate our algorithm , we collected and hand labeled a dataset of over 1000 web pages retrieved from google using personal name queries .our experimental results shows an improvement over the existing methods for namesake disambiguation . personal name disambiguation based on unsupervised algorithm
measuring semantic similarity between words using web search engines . semantic similarity measures play important roles in information retrieval and natural language processing .previous work in semantic web-related applications such as community mining , relation extraction , automatic meta data extraction have used various semantic similarity measures .despite the usefulness of semantic similarity measures in these applications , robustly measuring semantic similarity between two words ( or entities ) remains a challenging task .we propose a robust semantic similarity measure that uses the information available on the web to measure similarity between words or entities .the proposed method exploits page counts and text snippets returned by a web search engine .we define various similarity scores for two given words p and q , using the page counts for the queries p , q and p and q. moreover , we propose a novel approach to compute semantic similarity using automatically extracted lexico-syntactic patterns from text snippets .these different similarity scores are integrated using support vector machines , to leverage a robust semantic similarity measure .experimental results on miller-charles benchmark dataset show that the proposed measure outperforms all the existing web-based semantic similarity measures by a wide margin , achieving a correlation coefficient of 0.834 .moreover , the proposed semantic similarity measure significantly improves the accuracy ( f-measure of 0.78 ) in a community mining task , and in an entity disambiguation task , thereby verifying the capability of the proposed measure to capture semantic similarity using web content . automatic meta data extraction based on web - based semantic similarity measures
syntactic clustering of the web . we have developed an efficient way to determine the syntactic similarity of files and have applied it to every document on the world wide web .using this mechanism , we built a clustering of all the documents that are syntactically similar .possible applications include a " lost and found " service , filtering the results of web searches , updating widely distributed web-pages , and identifying violations of intellectual property rights . web searches based on clustering
adaptive graphical approach to entity resolution . entity resolution is a very common information quality ( iq ) problem with many different applications .in digital libraries , it is related to problems of citation matching and author name disambiguation ; in natural language processing , it is related to coreference matching and object identity ; in web application , it is related to web page disambiguation .the problem of entity resolution arises because objects / entities in real world datasets are often referred to by descriptions , which might not be unique identifiers of these entities , leading to ambiguity .the goal is to group all the entity descriptions that refer to the same real world entities .in this paper we present a graphical approach for entity resolution .it complements the traditional methodology with the analysis of the entity-relationship graph constructed for the dataset being analyzed .the paper demonstrates that a technique that measures the degree of interconnectedness between various pairs of nodes in the graph can significantly improve the quality of entity resolution .furthermore , the paper presents an algorithm for making that technique self-adaptive to the underlying data , thus minimizing the required participation from the domain-analyst and potentially further improving the disambiguation quality . natural language processing based on adaptive graphical approach
