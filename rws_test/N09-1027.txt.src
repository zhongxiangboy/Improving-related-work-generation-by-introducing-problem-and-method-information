0	a discriminative latent variable model for statistical machine translation . large-scale discriminative machine translation promises to further the state-of-the-art , but has failed to deliver convincing gains over current heuristic frequency count systems .we argue that a principle reason for this failure is not dealing with multiple , equivalent translations .we present a translation model which models derivations as a latent variable , in both training and decoding , and is fully discriminative and globally optimised .results show that accounting for multiple derivations does indeed improve performance .additionally , we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions . large - scale discriminative machine translation based on maximum conditional likelihood models
1	minimum bayes-risk decoding for statistical machine translation abstract . this statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word-to-word alignments from an mt system , and syntactic structure from parse-trees of source and target language sentences .we report the performance of the mbr decoders on a chinese-to-english translation task .our results show that mbr decoding can be used to tune statistical mt performance for specific loss functions . - to - english translation task based on minimum bayes - risk decoding
2	probabilistic cfg with latent annotations . this paper defines a generative probabilistic model of parse trees , which we call pcfg-la .this model is an extension of pcfg in which non-terminal symbols are augmented with latent variables .fine- grained cfg rules are automatically induced from a parsed corpus by training a pcfg-la model using an em-algorithm .because exact parsing with a pcfg-la is np-hard , several approximations are described and empirically compared .in experiments using the penn wsj corpus , our automatically trained model gave a performance of 86.6 % ( f , sentences 40 words ) , which is comparable to that of an unlexicalized pcfg parser created using extensive manual feature selection . exact parsing based on generative probabilistic model of parse trees
3	a better -best list : practical determinization of weighted finite tree automata . ranked lists of output trees from syntactic statistical nlp applications frequently contain multiple repeated entries .this redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes .it is chiefly due to nondeterminism in the weighted automata that produce the results .we introduce an algorithm that determinizes such automata while preserving proper weights , returning the sum of the weight of all multiply derived trees .we also demonstrate our algorithm ï¿½ s effectiveness on two large-scale tasks . syntactic statistical nlp applications based on weighted finite tree automata
4	learning accurate , compact , and interpretable tree annotation . we present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .starting with a simple x- bar grammar , we learn a new grammar whose non- terminals are subsymbols of the original nonterminals .in contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .on the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .despite its simplicity , our best grammar achieves an f , of 90.2 % on the penn treebank , higher than fully lexicalized systems . manual tree annotation based on fully lexicalized systems
5	lattice minimum bayes-risk decoding for statistical machine translation . we present minimum bayes-risk ( mbr ) decoding over translation lattices that compactly encode a huge number of translation hypotheses .we describe conditions on the loss function that will enable efficient implementation of mbr decoders on lattices .we introduce an approximation to the bleu score ( papineni et al. , 2001 ) that satisfies these conditions .the mbr decoding under this approximate bleu is realized using weighted finite state automata .our experiments show that the lattice mbr decoder yields moderate , consistent gains in translation performance over n-best mbr decoding on arabicto-english , chinese-to-english and englishto-chinese translation tasks .we conduct a range of experiments to understand why lattice mbr improves upon n-best mbr and study the impact of various parameters on mbr performance . statistical machine translation based on lattice minimum bayes - risk decoding
