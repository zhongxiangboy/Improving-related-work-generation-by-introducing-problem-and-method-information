using encyclopedic knowledge for named entity disambiguation . we present a new method for detecting and disambiguating named entities in open domain text .a disambiguation svm kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia .the resulting model significantly outperforms a less informed baseline . detecting and disambiguating named entities based on disambiguation svm kernel
large-scale named entity disambiguation based on wikipedia data . this paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and web search results .it describes in detail the disambiguation paradigm employed and the information extraction process from wikipedia .through a process of maximizing the agreement between the contextual information extracted from wikipedia and the context of a document , as well as the agreement among the category tags associated with the candidate entities , the implemented system shows high disambiguation accuracy on both news stories and wikipedia articles . large - scale named entity disambiguation based on disambiguation paradigm
computing semantic relatedness using wikipedia-based explicit semantic analysis . computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge .we propose explicit semantic analysis ( esa ) , a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from wikipedia .we use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of wikipedia-based concepts .assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics ( e.g. , cosine ) .compared with the previous state of the art , using esa results in substantial improvements in correlation of computed relatedness scores with human judgments : from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts .importantly , due to the use of natural concepts , the esa model is easy to explain to human users . computing semantic relatedness based on machine learning techniques
exploiting wikipedia as external knowledge for named entity recognition . we explore the use of wikipedia as external knowledge to improve named entity recognition ( ner ) .our method retrieves the corresponding wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry , which can be thought of as a definition part .these category labels are used as features in a crf-based ne tagger .we demonstrate using the conll 2003 dataset that the wikipedia category labels extracted by such a simple method actually improve the accuracy of ner . named entity recognition based on crf - based ne tagger
mining domain-specific thesauri from wikipedia : a case study . domain-specific thesauri are high-cost , high- maintenance , high-value knowledge structures .we show how the classic thesaurus structure of terms and links can be mined automatically from wikipedia , a vast , open encyclopedia .in a comparison with a professional thesaurus for agriculture ( agrovoc ) we find that wikipedia contains a substantial proportion of its domain-specific concepts and semantic relations ; furthermore it has impressive coverage of a collection of contemporary documents in the domain .thesauri derived using these techniques are attractive because they capitalize on existing public efforts and tend to reflect contemporary language usage better than their costly , painstakingly-constructed manual counterparts . mining domain - specific thesauri based on domain - specific thesauri
wikirelate ! abstract .wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than wordnet .in this work we present experiments on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets .existing relatedness measures perform better using wikipedia than a baseline given by google counts , and we show that wikipedia outperforms wordnet when applied to the largest available dataset designed for that purpose .the best results on this dataset are obtained by integrating google , wordnet and wikipedia based measures .we also show that including wikipedia improves the performance of an nlp application processing naturally occurring texts . nlp application based on wikipedia based measures
a proposal to automatically build and maintain gazetteers for named entity recognition by using wikipedia . this paper describes a method to automatically create and maintain gazetteers for named entity recognition ( ner ) .this method extracts the necessary information from linguistic resources .our approach is based on the analysis of an on-line encyclopedia entries by using a noun hierarchy and optionally a pos tagger .an important motivation is to reach a high level of language independence .this restricts the techniques that can be used but makes the method useful for languages with few resources .the evaluation carried out proves that this approach can be successfully used to build ner gazetteers for location ( f 78 % ) and person ( f 68 % ) categories . named entity recognition ( ner based on ner gazetteers
utilizing wikipedia categories for document classification . this paper introduces our technique for integrating wikipedia as a broad-coverage knowledge base for use in document classification .we outline an algorithm for integrating the wikipedia categories found from named entities in the articles .we then demonstrate this algorithm on a toy corpus , where we are able to successfully classify our documents . document classification based on 
analyzing and accessing wikipedia as a lexical semantic resource . in this paper , we analyze wikipedia as an emerginglexical semanticresource that is growing exponentially .recent research has shown thatwikipediacan be successfully employed for nlp tasks e.g. question answering ahnetet al. , 2004 ) , text classification ( gabrilovich and markovitch , 2006 ) or named entity disambiguation ( bunescu and pasca 2006 ) we extend this work by focusing on the analysis of wikipedia contentin particularitsitscategory structure , and present a highly efficient java-based api to usewikipediain large scale nlp . named entity disambiguation based on java - based api
